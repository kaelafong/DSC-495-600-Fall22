{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[What is Topic Modeling?](#What-is-Topic-Modeling?)   \n",
    "1. [How do topic models work?](#How-do-topic-models-work?)   \n",
    "2. [What are some Topic Modeling Techniques?](#What-are-some-Topic-Modeling-Techniques?)    \n",
    "    a. [Latent Semantic Analysis (LSA)](#Latent-Semantic-Analysis-(LSA))   \n",
    "    b. [Latent Dirichlet Allocation (LDA)](#Latent-Dirichlet-Allocation-(LDA))  \n",
    "    c. [Non-negative Matrix Factorization (NMF)](#Non-negative-Matrix-Factorization-(NMF))   \n",
    "\n",
    "\n",
    "# What is Topic Modeling?\n",
    "\n",
    "Topic modeling is a form of text mining, a way of identifying patterns in a corpus. You take your corpus and run it through a tool which groups words across the corpus into ‘topics’.\n",
    "\n",
    "What, then, is a topic? \n",
    "1. One definition offered on [Twitter during a conference](https://twitter.com/footnotesrising/status/264823621799780353) on topic modeling described a topic as “a recurring pattern of co-occurring words.” A topic modeling tool looks through a corpus for these clusters of words and groups them together by a process of similarity. In a good topic model, the words in topic make sense, for example “navy, ship, captain” and “tobacco, farm, crops.”\n",
    "\n",
    "![alt text](Tweet.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A topic model is a type of algorithm that scans a set of documents (known in the NLP field as a corpus), examines how words and phrases co-occur in them, and automatically “learns” groups or clusters of words that best characterize those documents. These sets of words often appear to represent a coherent theme or topic.\n",
    "\n",
    "3. Topic modeling is an unsupervised machine learning method that can scan the document sets and identify the phrase patterns and words inside them, and create a collection of the word groups and related expressions that rightly characterize the document sets. It gives us techniques to schedule, understand, and review textual big data.\n",
    "\n",
    "## How do topic models work?\n",
    "\n",
    "In one way or another, every __topic modeling__ algorithm starts with the assumption that your documents consist of a fixed number of topics. The model then assesses the underlying structure of the words within your data and attempts to find the groups of words that best “fit” your corpus based on that constraint.\n",
    "\n",
    "Another way to think about how __topic modeling__ works is to imagine working through an article with a set of highlighters. As you read through the article, you use a different color for the key words of themes within the paper as you come across them. When you were done, you could copy out the words as grouped by the color you assigned them. That list of words is a topic, and each color represents a different topic. \n",
    "\n",
    "![alt text](Illustration.png \"Illustration of Probabilistic Topic Models. (FromBlei, D. 2012)\")\n",
    "\n",
    "_Illustration of Probabilistic Topic Models. (FromBlei, D. 2012)_\n",
    "\n",
    "\n",
    "\n",
    "AI-powered text analysis uses a wide variety of methods or algorithms to process language naturally, one of which is __topic analysis__ – used to automatically detect topics from texts. \n",
    "\n",
    "By using __topic analysis__ models, businesses are able to offload simple tasks onto machines instead of overloading employees with too much data. Just imagine the time your team could save and spend on more important tasks, if a machine was able to sort through endless lists of customer surveys or support tickets every morning.\n",
    "\n",
    "__Topic modeling__ is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as __‘unsupervised’__ machine learning because it doesn’t require a predefined list of tags or training data that’s been previously classified by humans.\n",
    "\n",
    "Since __topic modeling__ doesn’t require training, it’s a quick and easy way to start analyzing your data. However, you can’t guarantee you’ll receive accurate results, which is why many businesses opt to invest time training a topic classification model.\n",
    "\n",
    "Since topic __classification__ models require training, they’re known as __‘supervised’__ machine learning techniques. What does that mean? Well, as opposed to text modeling, topic classification needs to know the topics of a set of texts before analyzing them. Using these topics, data is tagged manually so that a topic classifier can learn and later make predictions by itself.\n",
    "\n",
    "## What are some Topic Modeling Techniques?\n",
    "\n",
    "###  Latent Semantic Analysis (LSA)\n",
    "__Latent Semantic Analysis (LSA)__ is one of the most frequent topic modeling methods analysts make use of. It is based on what is known as the _distributional hypothesis_ which states that the semantics of words can be grasped by looking at the contexts the words appear in. In other words, under this hypothesis, the semantics of two words will be similar if they tend to occur in similar contexts.\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "__Latent Dirichlet Allocation (LDA)__ and LSA are based on the same underlying assumptions: the distributional hypothesis, (i.e. similar topics make use of similar words) and the statistical mixture hypothesis (i.e. documents talk about several topics) for which a statistical distribution can be determined. The purpose of LDA is mapping each document in our corpus to a set of topics which covers a good deal of the words in the document.\n",
    "\n",
    "![alt text](LSALDA.png \"Illustration of Probabilistic Topic Models. (FromBlei, D. 2012)\")\n",
    "\n",
    "### Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "__Non-Negative Matrix Factorization__ is a statistical method that helps us to reduce the dimension of the input corpora or corpora. Internally, it uses the factor analysis method to give comparatively less weightage to the words that are having less coherence. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
